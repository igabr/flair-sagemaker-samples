{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sagemaker Processing for post-processing aggregation\n",
    "\n",
    "There are situations when you need to perform post-processing on your batch predictions. A good example of such use case could be to calculate aggregate statistics, e.g. in case of Named Entity Recoknition task calculate how many times specific tokens occur in prediction dataset. \n",
    "\n",
    "Amazon Sagemaker allows you to easily implement this type of scenarios using [Sagemaker Processing feature](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html). In this notebook, we'll explore how to calculate number of total occurences for specific tokens using Sagemaker Processing.\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform. A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "\n",
    "![alt text](./images/proc_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building custom processing container\n",
    "\n",
    "For this exercise we will use a simple processing container built from scratch. This will give some idea how easy it is to port your existing processing code/pipeline and run it using Sagemaker capabilities.\n",
    "\n",
    "Let's define parameteres of our customer container, then build and push it to your account ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "ACCOUNT_ID = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "CONTAINER_NAME=\"custom-processing\"\n",
    "TAG = \"latest\"\n",
    "REGION = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin {ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m\u001b[33m python:3.7-slim-buster\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########### Installing packages ##########\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install \u001b[31mpandas\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.25.3 scikit-learn==\u001b[34m0\u001b[39;49;00m.21.3\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m PYTHONUNBUFFERED=TRUE\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########### Configure processing scripts ##########\u001b[39;49;00m\n",
      "ARG \u001b[31mcode_dir\u001b[39;49;00m=/opt/ml/code\n",
      "\u001b[34mRUN\u001b[39;49;00m mkdir -p \u001b[31m$code_dir\u001b[39;49;00m\n",
      "COPY processing_sources \u001b[31m$code_dir\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $code_dir\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m\u001b[33m [\"python3\",\"processing.py\"]\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker Dockerfile.postproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build container and push it to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build_and_push.sh $CONTAINER_NAME $TAG Dockerfile.postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following container will be used for processing job:  553020858742.dkr.ecr.us-east-2.amazonaws.com/custom-processing:latest\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URI = f\"{ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com/{CONTAINER_NAME}:{TAG}\"\n",
    "print(\"Following container will be used for processing job: \", IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing processing script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(args):\n",
      "    \n",
      "    tokens = args.tokens.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# initiate dict to store token counts results\u001b[39;49;00m\n",
      "    token_counts = {token : \u001b[34m0\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m tokens}\n",
      "    \n",
      "    files = []\n",
      "    \u001b[34mfor\u001b[39;49;00m _,_,filenames \u001b[35min\u001b[39;49;00m os.walk(args.input_dir):\n",
      "        files.extend(filenames)\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m \u001b[35min\u001b[39;49;00m files:\n",
      "        \n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[36mfile\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m:\n",
      "            file_string = \u001b[36mfile\u001b[39;49;00m.read().replace(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m token_counts.items():\n",
      "            token_counts[k] += file_string.count(k)\n",
      "    \n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtoken_counts.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m:\n",
      "        json.dump(token_counts, \u001b[36mfile\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Parse common arguments\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting training...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser = ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--tokens\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, required=\u001b[36mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mspecify which tokens to count in input files\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal dir with files for processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/processed_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal dir with files for processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "! pygmentize processing_sources/processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sagemaker Processing job\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"vadimd-batch-transform\"\n",
    "\n",
    "prefix_input = 'flair-output' \n",
    "prefix_output = 'postproc-output' \n",
    "\n",
    "input_s3_path = f\"s3://{bucket}/{prefix_input}\"    # S3 path where results of our inference job are stored\n",
    "output_s3_path = f\"s3://{bucket}/{prefix_output}\"  # S3 path where we'll upload aggregated processing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which tokens we'd like to count as part of processing. This dictionary will be passed as command line arguments to our processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_count = [\"<B-LOC>\", \"<E-LOC>\", \"<I-LOC>\", \"<S-LOC>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  nlp-processing-1-2020-06-02-00-11-10-089\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://vadimd-batch-transform/flair-output', 'LocalPath': '/opt/ml/processing/input_data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'S3Output': {'S3Uri': 's3://vadimd-batch-transform/postproc-output', 'LocalPath': '/opt/ml/processing/processed_data', 'S3UploadMode': 'EndOfJob'}}]\n",
      "............."
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = Processor(image_uri=IMAGE_URI,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      base_job_name=\"nlp-processing-1\",\n",
    "                      sagemaker_session=sess, \n",
    "                      instance_type=\"ml.m5.xlarge\")\n",
    "                     \n",
    "processor.run(inputs=[ProcessingInput(source=input_s3_path,\n",
    "                                      destination='/opt/ml/processing/input_data')],\n",
    "                                      outputs=[ProcessingOutput(source='/opt/ml/processing/processed_data',\n",
    "                                      destination=output_s3_path)],\n",
    "                                      arguments=[\"--tokens\", ','.join(tokens_to_count)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download file with results and check that results are as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://vadimd-batch-transform/postproc-output/token_counts.json to ./token_counts.json\n",
      "{\"<B-LOC>\": 4, \"<E-LOC>\": 4, \"<I-LOC>\": 1, \"<S-LOC>\": 4}"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://vadimd-batch-transform/postproc-output/token_counts.json token_counts.json\n",
    "! cat token_counts.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
