{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sagemaker Processing for post-processing aggregation\n",
    "\n",
    "There are situations when you need to perform post-processing on your batch predictions. A good example of such use case could be to calculate aggregate statistics, e.g. in case of Named Entity Recoknition task calculate how many times specific tokens occur in prediction dataset. \n",
    "\n",
    "Amazon Sagemaker allows you to easily implement this type of scenarios using [Sagemaker Processing feature](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html). In this notebook, we'll explore how to calculate number of total occurences for specific tokens using Sagemaker Processing.\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform. A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "\n",
    "![alt text](./images/proc_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building custom processing container\n",
    "\n",
    "For this exercise we will use a simple processing container built from scratch. This will give some idea how easy it is to port your existing processing code/pipeline and run it using Sagemaker capabilities.\n",
    "\n",
    "Let's define parameteres of our customer container, then build and push it to your account ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "ACCOUNT_ID = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "CONTAINER_NAME=\"custom-processing\"\n",
    "TAG = \"latest\"\n",
    "REGION = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin {ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m\u001b[33m python:3.7-slim-buster\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########### Installing packages ##########\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install \u001b[31mpandas\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.25.3 scikit-learn==\u001b[34m0\u001b[39;49;00m.21.3\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m PYTHONUNBUFFERED=TRUE\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########### Configure processing scripts ##########\u001b[39;49;00m\n",
      "ARG \u001b[31mcode_dir\u001b[39;49;00m=/opt/ml/code\n",
      "\u001b[34mRUN\u001b[39;49;00m mkdir -p \u001b[31m$code_dir\u001b[39;49;00m\n",
      "COPY processing_sources \u001b[31m$code_dir\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $code_dir\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m\u001b[33m [\"python3\",\"processing.py\"]\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker Dockerfile.postproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build container and push it to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  197.6kB\n",
      "Step 1/8 : FROM python:3.7-slim-buster\n",
      " ---> 87b1022604d5\n",
      "Step 2/8 : RUN pip3 install pandas==0.25.3 scikit-learn==0.21.3\n",
      " ---> Using cache\n",
      " ---> a5a5041b02a0\n",
      "Step 3/8 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 0fd4d5fe7349\n",
      "Step 4/8 : ARG code_dir=/opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 51db599d8da2\n",
      "Step 5/8 : RUN mkdir -p $code_dir\n",
      " ---> Using cache\n",
      " ---> 17de15081774\n",
      "Step 6/8 : COPY processing_sources $code_dir\n",
      " ---> 0845b73cf427\n",
      "Step 7/8 : WORKDIR $code_dir\n",
      " ---> Running in 683844f39f11\n",
      "Removing intermediate container 683844f39f11\n",
      " ---> ab7483d0f276\n",
      "Step 8/8 : ENTRYPOINT [\"python3\",\"processing.py\"]\n",
      " ---> Running in a1d2492c7e31\n",
      "Removing intermediate container a1d2492c7e31\n",
      " ---> f30816072101\n",
      "Successfully built f30816072101\n",
      "Successfully tagged custom-processing:latest\n",
      "The push refers to repository [553020858742.dkr.ecr.us-east-2.amazonaws.com/custom-processing]\n",
      "\n",
      "\u001b[1B551072fc: Preparing \n",
      "\u001b[1B9ea2966a: Preparing \n",
      "\u001b[1B98f5271e: Preparing \n",
      "\u001b[1B068e8a69: Preparing \n",
      "\u001b[1B464db597: Preparing \n",
      "\u001b[1B0ce58669: Preparing \n",
      "\u001b[1B6e8168dc: Preparing \n",
      "\u001b[8B551072fc: Pushed    7.68kBists 5kB\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[Klatest: digest: sha256:8ee6ccb397bc3f60a65c1d1a3e1dca57c8a8d83a65168d5abd531794192faec0 size: 1998\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh $CONTAINER_NAME $TAG Dockerfile.postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following container will be used for processing job:  553020858742.dkr.ecr.us-east-2.amazonaws.com/custom-processing:latest\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URI = f\"{ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com/{CONTAINER_NAME}:{TAG}\"\n",
    "print(\"Following container will be used for processing job: \", IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing processing script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(args):\n",
      "    \n",
      "    tokens = args.tokens.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# initiate dict to store token counts results\u001b[39;49;00m\n",
      "    token_counts = {token : \u001b[34m0\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m tokens}\n",
      "    \n",
      "    files = []\n",
      "    \u001b[34mfor\u001b[39;49;00m _,_,filenames \u001b[35min\u001b[39;49;00m os.walk(args.input_dir):\n",
      "        files.extend(filenames)\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m \u001b[35min\u001b[39;49;00m files:\n",
      "        \n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[36mfile\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m:\n",
      "            file_string = \u001b[36mfile\u001b[39;49;00m.read().replace(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m token_counts.items():\n",
      "            token_counts[k] += file_string.count(k)\n",
      "    \n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtoken_counts.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m \u001b[36mfile\u001b[39;49;00m:\n",
      "        json.dump(token_counts, \u001b[36mfile\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Parse common arguments\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting training...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser = ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--tokens\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, required=\u001b[36mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mspecify which tokens to count in input files\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal dir with files for processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/processed_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal dir with files for processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "! pygmentize processing_sources/processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sagemaker Processing job\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"vadimd-batch-transform\"\n",
    "\n",
    "prefix_input = 'flair-output' \n",
    "prefix_output = 'postproc-output' \n",
    "\n",
    "input_s3_path = f\"s3://{bucket}/{prefix_input}\"    # S3 path where results of our inference job are stored\n",
    "output_s3_path = f\"s3://{bucket}/{prefix_output}\"  # S3 path where we'll upload aggregated processing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which tokens we'd like to count as part of processing. This dictionary will be passed as command line arguments to our processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_count = [\"<B-LOC>\", \"<E-LOC>\", \"<I-LOC>\", \"<S-LOC>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  nlp-processing-2-2020-06-02-00-00-26-689\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://vadimd-batch-transform/flair-output', 'LocalPath': '/opt/ml/processing/input_data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'S3Output': {'S3Uri': 's3://vadimd-batch-transform/postproc-output', 'LocalPath': '/opt/ml/processing/processed_data', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...................\u001b[34mStarting training...\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = Processor(image_uri=IMAGE_URI,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      base_job_name=\"nlp-processing-2\",\n",
    "                      sagemaker_session=sess, \n",
    "                      instance_type=\"ml.m5.xlarge\")\n",
    "                     \n",
    "processor.run(inputs=[ProcessingInput(source=input_s3_path,\n",
    "                                      destination='/opt/ml/processing/input_data')],\n",
    "                                      outputs=[ProcessingOutput(source='/opt/ml/processing/processed_data',\n",
    "                                      destination=output_s3_path)],\n",
    "                                      arguments=[\"--tokens\", ','.join(tokens_to_count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: Parameter validation failed:\n",
      "Invalid bucket name \"s3:\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN matching the regex \"^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[/:][a-zA-Z0-9\\-]{1,63}$\"\n",
      "cat: token_counts.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://{output_s3_path}/token_counts.json token_counts.json\n",
    "! cat token_counts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
