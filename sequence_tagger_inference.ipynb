{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting FLAIR models on Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some initial imports and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import pickle\n",
    "import flair\n",
    "\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = \"us-east-2\"\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "prefix_input = 'flair-input'\n",
    "prefix_output = 'flair-ouput'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters of your container\n",
    "\n",
    "In this notebook, we are using PyTorch container provided by Sagemaker. Full list of available deep learning containers is here: https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html\n",
    "\n",
    "If needed, Sagemaker containers can be extended.  Typically, this is needed when you need to have a specific custom setup or when requirement.txt doesn't allow you to build/install desired version of Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following container will be used for hosting:  763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.5.0-gpu-py36-cu101-ubuntu16.04\n"
     ]
    }
   ],
   "source": [
    "# Using Sagemaker PyTorch 1.5 serving container\n",
    "\n",
    "container_serving = \"pytorch-inference\" # your container name\n",
    "tag = \"1.5.0-gpu-py36-cu101-ubuntu16.04\" # you can have several version of container available\n",
    "image = '763104351884.dkr.ecr.{}.amazonaws.com/{}:{}'.format(region, container_serving, tag)\n",
    "\n",
    "print(\"Following container will be used for hosting: \",image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Realtime endpoint\n",
    "\n",
    "Sagemaker Realtime endpoints allows you to get inference in real-time by sending HTTP requests. In this example, we'll deploy a new endpoint. Inference code is provided in `sequence_tagger_serving.py` file, `requirements.txt` specify which additional Python packages you need to install additionally in Sagemaker PyTorch container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel, PyTorch, PyTorchPredictor\n",
    "from sagemaker.estimator import Estimator, Model\n",
    "\n",
    "remote_model = PyTorchModel(name = \"flair-tagger-v3\",\n",
    "                            model_data=\"s3://vadimd-empty-bucket/placeholder/emptyfile.tar.gz\", # This will be replaced with location of trained model artifacts. \n",
    "                                                                                                # For now, default pre-trained FLAIR model will be used.\n",
    "                            role=role,\n",
    "                            sagemaker_session = sess,\n",
    "                            entry_point=\"sequence_tagger_serving.py\",\n",
    "                            source_dir=\"serving_sources\",\n",
    "                            framework_version=\"1.5\", py_version=\"3.6\",\n",
    "                            image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor = remote_model.deploy(instance_type='ml.g4dn.xlarge',\n",
    "                                       initial_instance_count=1,\n",
    "                                       update_endpoint = True, # comment or False if endpoint doesns't exist\n",
    "                                       endpoint_name=\"sequence-endpoint-v1\", # define a unqie endpoint name; if ommited, Sagemaker will generate it based on used container\n",
    "                                       wait=False\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence: \"New York City (NYC), often called The City or simply New York (NY), is the most populous city in the United States.\"   [− Tokens: 22  − Token-Labels: \"New <B-LOC> York <I-LOC> City <E-LOC> (NYC), often called The <B-LOC> City <E-LOC> or simply New <B-LOC> York <E-LOC> (NY), is the most populous city in the United <B-LOC> States. <E-LOC>\"]]\n"
     ]
    }
   ],
   "source": [
    "# sentence_to_predict = \"Berlin is the capital and largest city of Germany by both area and population.\"\n",
    "sentence_to_predict  = \"New York City (NYC), often called The City or simply New York (NY), is the most populous city in the United States.\"\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "content_type = 'application/json'\n",
    "accept_type = \"pickle\"\n",
    "headers = {'content-type': content_type}\n",
    "payload = json.dumps(sentence_to_predict)\n",
    "endpoint_name = \"sequence-endpoint-v1\"\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType=content_type,\n",
    "    Accept = accept_type\n",
    ")\n",
    "\n",
    "prediction_obj = pickle.loads(response['Body'].read())\n",
    "print(prediction_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Batch Transform inference\n",
    "\n",
    "Sagemaker also allows to run inference in a batch format for multiple inputs. In this case, your input data is stored in S3 object, the output file with batch predictions will be also available as object on S3. \n",
    "\n",
    "We'll use the same container image as for realtime inference. The only difference, that this time both request content type and response content type will be \"text/csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"vadimd-batch-transform\"\n",
    "\n",
    "input_data_path = 's3://{}/{}/{}'.format(bucket, 'flair', 'sample.csv')\n",
    "output_data_path = 's3://{}/{}'.format(bucket, 'flair-output')\n",
    "\n",
    "transform_job = sagemaker.transformer.Transformer(\n",
    "    model_name = \"flair-tagger-v3\",\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.p2.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "#     base_transform_job_name='inference-pipelines-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = \"text/csv\")\n",
    "\n",
    "transform_job.transform(data = input_data_path, \n",
    "                        content_type = \"text/csv\", \n",
    "                        split_type = 'Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After batch transform job successfully "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
